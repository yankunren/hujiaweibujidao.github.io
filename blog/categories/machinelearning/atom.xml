<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: machinelearning | Hujiawei Bujidao]]></title>
  <link href="http://hujiaweibujidao.github.io/blog/categories/machinelearning/atom.xml" rel="self"/>
  <link href="http://hujiaweibujidao.github.io/"/>
  <updated>2014-07-04T17:21:48+08:00</updated>
  <id>http://hujiaweibujidao.github.io/</id>
  <author>
    <name><![CDATA[hujiawei]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Dog Face Recognition]]></title>
    <link href="http://hujiaweibujidao.github.io/blog/2014/06/09/Dog-Face-Recognition/"/>
    <updated>2014-06-09T21:30:00+08:00</updated>
    <id>http://hujiaweibujidao.github.io/blog/2014/06/09/Dog-Face-Recognition</id>
    <content type="html"><![CDATA[<h3 id="center-center"><center>模式识别大作业-狗脸识别</center></h3>
<center> 胡家威 计研135班 2013210902 </center>

<h4 id="section">1. 题目要求</h4>

<h5 id="pca">(1)PCA狗脸识别</h5>

<p>采用PCA狗脸识别的方法完成下面的实验。图像特征可以采用灰度像素值、颜色直方图等。</p>

<p>1.用每个品种的一半数据做训练,另一半数据做测试(可以前40张图像作为训练，后40张图像作为测试)。采用K近邻分类(必做:K=1，选做:K=3,5)，分析选取不同的主分量个数，对识别率和虚警率的影响。</p>

<p>2.评价该方法的性能</p>

<p>3.计算每个品种的正确识别率</p>

<p>4.进行开集测试(见题目要求3)</p>

<h5 id="fisher">(2)Fisher狗脸识别</h5>

<p>采用线性判别准则的方法进行实验。</p>

<p>1.用每个品种的一半数据做训练，另一半数据做测试(可以前40张图像作为训练，后40张图像作为测试)。给出用fisherface方法得到的识别率。</p>

<p>2.同PCA一样，评价该方法的性能 </p>

<p>3.进行开集测试(见题目要求3)</p>

<p>4.比较这两个方法的优缺点</p>

<h5 id="section-1">(3)开集测试</h5>

<p>图像数据中还有80张负样本(neg，猫脸)，即非狗脸图像。此时需要给出一个合理的拒识方式来判断 某张图像是否属于训练的10个品种。请设计一个合理的拒识方式(最简单的方式是对测试图像到训练 图像的最近距离设定一个阈值)并对400张狗脸测试图像和80张猫脸测试图像进行识别(11个类别，最后一个为neg类)，观察阈值不同时对识别结果的影响。</p>

<h5 id="section-2">(4)选作部分</h5>

<p>1.(选做)根据上面的评价比较，给出改善，并且对新方法再进行评价</p>

<p>2.可以采用更加复杂的特征如HOG，BOW特征，也可以在分类方法上采用别的方式(如SVM、层级式分类)而不是K邻分类。鼓励同学们创新。</p>

<p>3.需要有曲线，表格和测试数据的说明</p>

<h4 id="section-3">2.实验内容</h4>

<p>实验总体介绍：本次实验我共尝试使用了三种不同的图像特征进行比较：(1)灰度像素值； (2)LBP特征； (3)HOG特征。每一种图像特征又结合下面四种算法：(1)PCA； (2)Fisher； (3)SVM； (4)HOSVD 来进行分类，并且采用了开集测试和10折交叉验证的方式分析算法的正确率。下面是实验的分析过程和分析结果</p>

<h5 id="section-4">2.1 特征选择和提取</h5>

<p>下图显示了要识别的10个品种的狗，每个品种的狗只选择了其中一张图像，下文中提到的狗的品种编号按照下图中从左到右从上到下的方式进行索引。</p>

<p><img src="/images/ml/gallery_dog_exp.jpg" alt="image" /></p>

<p>下图显示了实验使用的三种特征，从左到右分别是原图(第一条狗的第71张图像)以及它对应的灰度图、LBP特征图、HOG特征图，得到的数据分别存储在数据结构gray,lbp和hog中，然后保存为mat格式的文件，因为数据量比较大，每次重新提取会耗费大量时间，所以采用先保存在需要的时候再进行加载的方式。对于一张大小为144x144的图像，灰度像素特征共有144x144个；LBP特征采用的半径是1，取8个领域，所以特征共有142x142个；HOG特征采用的cell的大小为9，所以特征共有9x9=81个。</p>

<p><img src="/images/ml/dog_4imgs.png" alt="image" /></p>

<h5 id="section-5">2.2 特征值和特征脸的观察</h5>

<p>下图显示了对所有灰度图像的特征值进行分析得到的结果，左图显示了各个特征值在总特征值之和中所占的比例，很明显只有前面几个特征值具有较高的比例，后面的特征基本上都是冗余的；右图显示了特征值的累计之和在总特征值之和中所占的比例，通过观察也不难发现前面一些特征的累加便可达到90%以上的比例。</p>

<p><img src="/images/ml/eigen.png" alt="image" /></p>

<p>假设保证使用90%作为选择主成分数目的阈值标准，那么对于灰度像素特征，通过计算得到共需要前95个特征。对于LBP特征和HOG特征同样可以进行上面的分析，下面是HOG特征的结果，因为总共只有81个特征，所以计算很快，但是要达到90%以上的比例需要29个特征。另外，LBP特征因为它的特殊性，它需要649个特征才能达到90%以上的比例。</p>

<p><img src="/images/ml/eigen_hog.png" alt="image" /></p>

<p>下图显示了灰度特征下得到的狗的前16个特征脸，为了增强可视性，我使用了方法<code>colormap(jet(256))</code>提高显示效果。从结果中可以看出，第一个特征脸就很像一只小狗，其他的特征脸因为抓住的是狗脸的其他特征所以只是隐隐约约可见狗脸的轮廓。</p>

<p><img src="/images/ml/eigenface_gray.png" alt="image" /></p>

<p>同样的，也可以查看下LBP特征下的特征脸，下面是前16个特征脸的显示结果，从结果中可以看出，LBP很好地抓住了狗脸部眼睛和鼻子的特征。</p>

<p><img src="/images/ml/eigenface_lbp.png" alt="image" /></p>

<h5 id="section-6">2.3 性能测试方式选择</h5>

<p>本次实验尝试了两种性能评价方法，一种是题目要求的取前40张图像作为训练，取后40张图像作为测试；另一种是常用的10折交叉验证的方法，每次从每个品种中选择8张(80/10=8)图像作为测试，其他的72张图像作为训练。例如，对于HOG特征，采用PCA和K近邻算法结合的狗脸识别得到的结果如下，可以看出一般开集测试的准确率都低于它对应的正样本测试得到的准确率，此外，采用交叉验证得到的准确率也要略高于第一种性能测试方式。另外，从理论上来说，交叉验证的性能评价方式更加科学，结果准确性更高，所以后面的结果大部分都采用交叉验证的性能测试方式，只是对于SVM和HOSVD这类复杂的运行时间长的算法采用第一种性能测试方式。</p>

<p><img src="/images/ml/hog_pca_k.png" alt="image" /></p>

<h5 id="section-7">2.4 开集测试的阈值选择</h5>

<p>上面的结果使用的阈值是负样本中的采用欧式距离度量情况下得到的最大距离M和最小距离m的平均值(M+m)/2，实验过程中我只尝试了两种不同的阈值比较，一个是使用最大距离M(使用最小距离m的话效果非常差，不作为比较之中)；另一种便是使用均值(M+m)/2。下面是使用HOG特征在PCA和最近邻算法下得到的结果，左图是使用均值的情况，右图是使用最大距离的情况。两者的准确率分别是58.09%和62.05%，最大距离稍微高些，但是仔细观察最后一行和最后一列，对于使用均值距离容易出现很多狗被认为不是狗，对于最大距离容易出现很多猫被认为是狗！相比较而言，我认为后者的风险更大，所以我采用的阈值是均值。</p>

<p><img src="/images/ml/cv_threshold.png" alt="image" /></p>

<h5 id="pca-1">2.5 PCA狗脸识别</h5>

<p>如果使用PCA以及K近邻算法进行狗脸识别，采用10折交叉验证方法对测试算法性能，对于不同的图像特征和K的取值得到下面的结果。从结果中可以看出，HOG特征的结果最优(平均都在60%以上)，其次是灰度像素特征(稳定在50%左右)，表现最差的是LBP特征(不超过30%)；在运行速度方面，HOG特征的运行速度最快，灰度像素特征和LBP特征的运行速度都比较慢。此外，对于不同的k近邻，HOG特征的结果差别不大，相当稳定；灰度像素特征受影响比较大，因为样本中图像的灰度差别比较大，同一个品种的狗的图像的灰度差别也比较大，甚至有些品种的狗本身就存在多种肤色的情况。[结果<code>A/B</code>分别表示对应的正样本测试和开集测试下的准确率，下同]</p>

<p><img src="/images/ml/pca_k.png" alt="image" /></p>

<p>下图是对于灰度像素特征得到的性能图，左边是正样本测试的结果，右边是开集测试的结果。从图中可以看出，判断错误的情况还是比较多的，尤其是比较多的错判为2和5两个品种的狗，这两个品种的狗都是褐色的，在狗的颜色当中比较具有代表性。</p>

<p><img src="/images/ml/gray_cv.png" alt="image" /></p>

<p>下图是对于LBP特征得到的性能图，左边是正样本测试的结果，右边是开集测试的结果。从图中可以看出，很多狗都被错误地判断为1、5和10三个品种的狗，观察发现这三种狗的脸部形状、眼睛和鼻子的分布在狗当中比较具有代表性。</p>

<p><img src="/images/ml/lbp_cv.png" alt="image" /></p>

<p>下图是对于HOG特征得到的性能图，左边是正样本测试的结果，右边是开集测试的结果。从图中可以看出，后面4个品种的狗的脸部相似性比较高，观察发现这四种狗的毛发比较多，因为导致它们的HOG特征相似性比其他品种的狗要高一些。</p>

<p><img src="/images/ml/hog_cv.png" alt="image" /></p>

<p>下图是HOG特征在k=1的情况得到的各个品种的狗的识别准确率 [准确率比较容易得到，下面就不显示该类结果了]</p>

<p><img src="/images/ml/precision.png" alt="image" /></p>

<h5 id="fisher-1">2.6 Fisher狗脸识别</h5>

<p>如果使用LDA以及K近邻算法进行狗脸识别，采用10折交叉验证方法对测试算法性能，对于不同的图像特征和K的取值得到下面的结果。HOG特征只有81个，数目小于N-C=((720-80)-10)=630，所以不能使用。从结果中可以看出，使用Fisher识别算法采用LBP特征比灰度像素特征更好些，在运行速度方面，两者的的运行速度差不多，但都比较慢。此外，对于不同的k近邻，灰度像素特征此时受影响程度不大，LBP特征受影响程度也不大。</p>

<p><img src="/images/ml/lda_k.png" alt="image" /></p>

<p>下图是对于灰度像素特征得到的性能图，左边是正样本测试的结果，右边是开集测试的结果。从图中可以看出，判断错误的情况基本上都是错判为品种5的狗，原因可能是品种5的狗在狗中是最为常见最为标准的样子。</p>

<p><img src="/images/ml/gray_lda_cv.png" alt="image" /></p>

<p>下图是对于LBP特征得到的性能图，左边是正样本测试的结果，右边是开集测试的结果。从图中可以看出，LBP特征在Fisher识别中的效果比PCA识别中的效果好很多，而且，通过右图可知拒识率相当高，也就是说虽然使用Fisher识别比PCA识别准确率高了不少，但是虚警率同样高了很多。</p>

<p><img src="/images/ml/lbp_lda_cv.png" alt="image" /></p>

<!--
下图是对于HOG特征得到的性能图，左边是正样本测试的结果，右边是开集测试的结果。从图中可以看出，后面4个品种的狗的脸部相似性比较高，观察发现这四种狗的毛发比较多，因为导致它们的HOG特征相似性比其他品种的狗要高一些。

![image](/images/ml/hog_lda_cv.png)
-->

<p>下图是灰度像素特征下得到的Fisher脸，下图的结果不容易看出的结果，部分图像隐约可见某个品种的狗脸轮廓，其中包括了狗的眼睛和鼻子。</p>

<p><img src="/images/ml/fisher_faces.png" alt="image" /></p>

<p><strong>与PCA狗脸识别对比</strong>：PCA识别的优点是速度快，Fisher略微慢些；在PCA中LBP特征准确率非常差，但是在Fisher中LBP特征的结果相当不错，超过了灰度像素特征；对于灰度像素特征，Fisher识别的准确率要比PCA识别的准确率要高，但两者将狗识别为非狗的情况都特别多；运行时间方面，两者的运行时间差不多。</p>

<h5 id="svm">2.7 SVM算法狗脸识别</h5>

<p>实验的过程中我分别测试了线性(linear)、多项式(polynomial)和径向基(RBF)三种不同核函数的SVM算法，经过测试之后我将参数C统一设置为20，以下是不同特征和核函数得到的结果。此外，因为SVM算法的训练需要比较长的时间，所以这里就不采用交叉验证的方式，而是采用第一种性能测试方式(一半训练另一半测试)。很明显，不同情况下的结果差别很大，例如，对于HOG特征，在线性SVM中得到的结果最好，接近90%，但是对于多项式和径向基核函数只能得到10%的准确率，这也说明了线性SVM虽然是最简单的SVM，但是在特定情况下没准是性能最好的SVM。 [尚不清楚三种特征在多项式核函数和径向基核函数的情况下都是10%的原因，不排除是不合适的参数造成]</p>

<p><img src="/images/ml/svm.png" alt="image" /></p>

<h5 id="hosvd">2.8 HOSVD算法狗脸识别</h5>

<p>为了做进一步扩展，我选择了HOSVD算法，即高维奇异值分解算法来和其他算法的性能进行对比。HOSVD算法是SVD算法在高维空间的扩展，在人脸识别领域使用的也比较多，相关知识参考书籍《Matrix Methods in Data Mining and Pattern Recognition》。</p>

<p>PCA算法虽然实现上比较简单,但是在不同的环境(例如光照条件)和不同的人物表情下识别率比较低，所以后来就演变出了“张量脸”算法，即HOSVD算法，它基于下图所示的张量的Tuker分解。简言之，就是将狗脸图像的某种特征视为一个列向量，将同一个品种的狗的不同图像(视为狗的不同的表情)对应的列向量组合成一个矩阵，然后将不同品种的狗对应的矩阵组合成如下图所示的张量x。HOSVD算法就是对张量x进行分解，得到一个核心张量和几个其他正交阵，更加详细的讲解参考上面书籍。</p>

<p><img src="/images/ml/tensor_turker.png" alt="image" /></p>

<p>与PCA类似，它也需要给定一个主成分的参数，如果张量的维数太大的话计算机训练一个HOSVD的模型需要比较长的时间，所以，为了保证HOSVD算法能够在一定的时间内给出结果，它的输入图像不是原始的大小，而是缩小为72x72，在这样的情况下完成灰度像素特征下的HOSVD分解大概需要8分钟。考虑到运行时间过长，所以此时也改用第一种性能测试方式(一半训练另一半测试)。下面是不同的特征和k值得到的HOSVD算法的结果 [画线处表示结果未计算，可以看出其结果并不是最优的]</p>

<p><img src="/images/ml/hosvd.png" alt="image" /></p>

<h4 id="section-8">3 实验总结</h4>

<p>本实验从特征的选择和提取开始，一步步经过特征值的分析以及对于性能评估方法和开集测试的阈值的选择，然后依次使用PCA识别、Fisher识别、SVM算法和HOSVD算法进行识别，分析算法的结果及其性能。通过对比各种算法的平均情况下的最好的结果，得到如下结果。从对比中可以看出，平均情况下表现最好的是SVM算法，而且是在HOG特征以及线性核函数的情况下表现最优；对于PCA识别，采用HOG特征最佳；对于Fisher识别，采用LBP特征最佳；而HOSVD算法需要的时间比SVM算法还多，但是性能并没有进一步提升，所以并不是该类问题的很好解决方案。</p>

<p><img src="/images/ml/all.png" alt="image" /></p>

<p><strong>实验评价</strong>：</p>

<p>(1)本实验的最大亮点在于很好地对多个特征在多个不同算法上的结果进行分析和比较，而且大胆尝试了SVM算法和HOSVD算法，但是由于对这两个略微复杂的算法的理解能力不足，没能更好地解释得到的实验结果(例如多项式和径向基核函数会得到很低的准确率的原因)，也没能进一步通过调整参数使得这两个算法的性能进一步提升；</p>

<p>(2)实验过程中有效地结合使用两种性能测试方式缩短了实验时间(SVM和HOSVD算法使用一半训练另一半测试的方式)，但是在对算法的性能进行比较的时候没能详细地比较具体的运行时间；</p>

<p>(3)实验过程中的分析还算是比较清晰，按照一定的逻辑不断调整策略，但是在一些阈值的选择(例如开集测试的距离阈值)、参数的选择(SVM算法的核函数的参数)方面感觉没有方向性，不能按照一定的思路朝着更好的结果进行。另外，由于缺乏数字图像处理的能力，没能对图像进行一些预处理操作，例如截取狗脸的核心区域不但可以减少特征数目，而且肯定能够提高算法的准确率。</p>

<h4 id="section-9">参考资料：</h4>

<p>1.Pattern Recognition. Sergios Theodoridis</p>

<p>2.Introduction to Pattern Recognition: A Matlab Approach. Sergios Theodoridis</p>

<p>3.Matrix Methods in Data Mining and Pattern Recognition. Lars Eldén</p>

<p>4.Blog of Bytefish: <a href="http://bytefish.de/blog">http://bytefish.de/blog</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PageRank]]></title>
    <link href="http://hujiaweibujidao.github.io/blog/2014/05/12/algorithms-pagerank/"/>
    <updated>2014-05-12T18:12:00+08:00</updated>
    <id>http://hujiaweibujidao.github.io/blog/2014/05/12/algorithms-pagerank</id>
    <content type="html"><![CDATA[<p><strong><center>ML/DM/PR算法系列</center></strong>
<strong><center>逸夫图书馆, 2014/5/17</center></strong></p>

<h3 id="centerpagerankcenter"><center>PageRank算法</center></h3>

<p>PageRank算法是谷歌曾经独步天下的“倚天剑”，该算法由Larry Page和Sergey Brin在斯坦福大学读研时发明的，<a href="http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf">论文点击下载: The PageRank Citation Ranking: Bringing Order to the Web</a>。</p>

<p>本文首先通过一些参考文献引出问题，然后给出了PageRank的几种实现算法，最后将其推广至在MapReduce框架下如何实现PageRank算法。</p>

<p>PageRank的核心思想有2点：</p>

<p>1.如果一个网页被很多其他网页链接到的话说明这个网页比较重要，也就是pagerank值会相对较高；</p>

<p>2.如果一个pagerank值很高的网页链接到一个其他的网页，那么被链接到的网页的pagerank值会相应地因此而提高。</p>

<p>下面是一张来自<a href="http://en.wikipedia.org/wiki/PageRank">WikiPedia</a>的图，每个球代表一个网页，球的大小反应了网页的pagerank值的大小。指向网页B和网页E的链接很多，所以B和E的pagerank值较高，另外，虽然很少有网页指向C，但是最重要的网页B指向了C，所以C的pagerank值比E还要大。</p>

<p><img src="http://hujiaweibujidao.github.io/images/algos/pagerank-fig.png" alt="image" /></p>

<p>参考内容：</p>

<p>1.<a href="http://en.wikipedia.org/wiki/PageRank">Wiki about PageRank</a></p>

<p>2.<a href="http://www.itlearner.com/good/pagerank_cn.htm">Google 的秘密- PageRank 彻底解说 中文版</a></p>

<p>3.<a href="http://book.douban.com/subject/7161824/">数值分析与算法</a> Page 161 应用实例：Google的PageRank算法</p>

<p>4.<a href="http://www.mathworks.cn/moler/chapters.html">Numeric Methods with Matlab</a> 或者<a href="http://book.douban.com/subject/1836464/">中文翻译版本Matlab数值计算</a></p>

<p>5.<a href="http://www.chenjunlu.com/2012/10/pagerank-on-mapreduce/">使用 MapReduce 思想计算 PageRank</a> Page 62 PageRank和马尔可夫链</p>

<h3 id="section">1.问题背景</h3>

<p>来自参考内容3</p>

<p><img src="http://hujiaweibujidao.github.io/images/algos/pk-1.png" alt="image" /></p>

<h3 id="section-1">2.数学建模</h3>

<p>来自参考内容3，理解网页连接矩阵$G$，马尔科夫过程(“网上冲浪”)，转移矩阵$A$，概率$p$为用户点击当前网页中的某个链接地址的概率(一般都为0.85)。</p>

<p><img src="http://hujiaweibujidao.github.io/images/algos/pk-2.png" alt="image" />
<img src="http://hujiaweibujidao.github.io/images/algos/pk-3.png" alt="image" /></p>

<p>最后得到一个等式$Ax=x$，这实际上就是求矩阵$A$的特征值为1的特征向量！</p>

<p>下面的内容使用圆盘定理解释了1是矩阵$A$的主特征值，所以我们可以使用幂法来求解。</p>

<p>关于幂法的详细介绍参考另一篇文章<a href="http://hujiaweibujidao.github.io/blog/2014/04/23/numerical-methods-using-matlab/">Numerical Methods Using Matlab: 第三章 矩阵特征值和奇异值求解</a></p>

<p><img src="http://hujiaweibujidao.github.io/images/algos/pk-4.png" alt="image" />
<img src="http://hujiaweibujidao.github.io/images/algos/pk-5.png" alt="image" /></p>

<!--
![image](http://hujiaweibujidao.github.io/images/algos/pk-6.png)
-->

<h3 id="pagerank">3.求解PageRank</h3>

<p>假设有如上图右侧所示的网页链接模型。</p>

<p>(1) 幂法</p>

<p>wiki上有一个PageRank的简便算法，它不考虑转移概率，而是采用的是迭代的方式，每次都更新所有网页的pagerank值，更新的方式就是将每个网页的pagerank值平摊分给它指向的所有网页，每个网页累计所有指向它的网页平摊给它的值作为它该回合的pagerank值，直到全部网页的pagerank值收敛了或者满足一定的阈值条件就停止。</p>

<p>后面的MapReduce框架下PageRank算法的实现就采用了这个思想。考虑转移概率的情况和这个算法类似，乘上一个转移概率再加上一个随机跳转的概率。</p>

<p><img src="http://hujiaweibujidao.github.io/images/algos/pagerank-Simplified-algorithm.png" alt="image" /></p>

<p>根据上面的思想，下面Matlab代码实现可以得到各个网页的PageRank值。</p>

<p>```matlab
n=6;
i=[2 3 4 4 5 6 1 6 1];
j=[1 2 2 3 3 3 4 5 6];
G=sparse(i,j,1,n,n);</p>

<p>% Power method
for j = 1:n
   L{j} = find(G(:,j));
   c(j) = length(L{j});
end</p>

<p>p = .85;
delta = (1-p)/n;
x = ones(n,1)/n;
z = zeros(n,1);
cnt = 0;
while max(abs(x-z)) &gt; .0001
   z = x;
   x = zeros(n,1);
   for j = 1:n
      if c(j) == 0
         x = x + z(j)/n;%转移到任意一个网页
      else
         x(L{j}) = x(L{j}) + z(j)/c(j);%将上次的pagerank值平摊给所有指向的网页
      end
   end
   x = p*x + delta;
   cnt = cnt+1;
end
```
得到的向量$x$保存了各个网页的pagerank值，虽然链接数目一样，但是网页①比网页④和网页⑤都高，而网页②的pagerank值第二高，因为网页①链接到了它上面，相当于沾了网页①的光。</p>

<p><code>
x =
    0.2675
    0.2524
    0.1323
    0.1698
    0.0625
    0.1156
</code></p>

<p><a href="http://www.chenjunlu.com/2012/09/pagerank-algorithm-implemented-in-python/">这篇文章给出该算法的一个Python版本实现</a>，该博主使用第三方模块<a href="https://code.google.com/p/python-graph/">python-graph</a>，python-graph模块实现了很多图算法，<a href="https://code.google.com/p/python-graph/wiki/Example">该模块的使用示例</a>，使用前需要先安装，代码如下：</p>

<p><code>python
easy_install python-graph-core
easy_install python-graph-dot
</code></p>

<p>Python版本的算法实现：</p>

<p>```
# coding=utf-8</p>

<h1 id="python-graph-httpscodegooglecomppython-graph">python-graph https://code.google.com/p/python-graph/</h1>

<h1 id="import-graphviz">Import graphviz</h1>
<p>import graphviz as gv</p>

<h1 id="import-pygraph">Import pygraph</h1>
<p>from pygraph.classes.digraph import digraph
from pygraph.readwrite.dot import write</p>

<h1 id="define-pagerank-function">Define pagerank function</h1>
<p>def pagerank(graph, damping_factor=0.85, max_iterations=100, \
             min_delta=0.00001):
    “””
    Compute and return the PageRank in an directed graph.</p>

<pre><code>@type  graph: digraph
@param graph: Digraph.

@type  damping_factor: number
@param damping_factor: PageRank dumping factor.

@type  max_iterations: number
@param max_iterations: Maximum number of iterations.

@type  min_delta: number
@param min_delta: Smallest variation required for a new iteration.

@rtype:  Dict
@return: Dict containing all the nodes PageRank.
"""

nodes = graph.nodes()
graph_size = len(nodes)
if graph_size == 0:
    return {}
# value for nodes without inbound links
min_value = (1.0-damping_factor)/graph_size

# itialize the page rank dict with 1/N for all nodes
#pagerank = dict.fromkeys(nodes, 1.0/graph_size)
pagerank = dict.fromkeys(nodes, 1.0)

for i in range(max_iterations):
    diff = 0 #total difference compared to last iteraction
    # computes each node PageRank based on inbound links
    for node in nodes:
        rank = min_value
        for referring_page in graph.incidents(node):
            rank += damping_factor * pagerank[referring_page] / \
                    len(graph.neighbors(referring_page))

        diff += abs(pagerank[node] - rank)
        pagerank[node] = rank

    print 'This is NO.%s iteration' % (i+1)
    print pagerank
    print ''

    #stop if PageRank has converged
    if diff &lt; min_delta:
        break

return pagerank
</code></pre>

<h1 id="graph-creation">Graph creation</h1>
<p>gr = digraph()</p>

<h1 id="add-nodes-and-edges">Add nodes and edges</h1>
<p>gr.add_nodes([“1”,”2”,”3”,”4”])</p>

<p>gr.add_edge((“1”,”2”))
gr.add_edge((“1”,”3”))
gr.add_edge((“1”,”4”))
gr.add_edge((“2”,”3”))
gr.add_edge((“2”,”4”))
gr.add_edge((“3”,”4”))
gr.add_edge((“4”,”2”))</p>

<h1 id="draw-as-png">Draw as PNG</h1>
<p># dot = write(gr)
# gvv = gv.readstring(dot)
# gv.layout(gvv,’dot’)
# gv.render(gvv,’png’,’Model.png’)</p>

<p>pagerank(gr)
```</p>

<p>经过32次迭代之后得到的结果如下，和前面的结果一致：</p>

<p><code>
This is NO.32 iteration
{'1': 0.2675338708706491, '3': 0.13227261904986046, '2': 0.2524037902400518, '5': 0.062477242064127136, '4': 0.1697488529161491, '6': 0.1155828978186352}
</code></p>

<p>(2) 利用马尔可夫矩阵的特殊结构</p>

<p>来自参考内容4，其中$\delta=\frac{1-p}{n}$</p>

<p><img src="http://hujiaweibujidao.github.io/images/algos/pk-8.png" alt="image" /></p>

<p>也就是将矩阵$A$进行分解，并不需要显示求出矩阵$A$，然后便是求解一个线性方程组即可。</p>

<p>```matlab
function x = pagerank1(G)
% PAGERANK1  Google’s PageRank modified version 1 - hujiawei</p>

<p>%if nargin &lt; 3, p = .85; end
p=0.85;</p>

<p>% Eliminate any self-referential links</p>

<p>G = G - diag(diag(G));</p>

<p>% c = out-degree, r = in-degree</p>

<p>[n,n] = size(G);
c = sum(G,1);%each row’s sum
r = sum(G,2);%each col’s sum</p>

<p>% Scale column sums to be 1 (or 0 where there are no out links).</p>

<p>k = find(c~=0);
D = sparse(k,k,1./c(k),n,n);</p>

<p>% Solve (I - p<em>G</em>D)*x = e</p>

<p>e = ones(n,1);
I = speye(n,n);
x = (I - p<em>G</em>D)\e;</p>

<p>% Normalize so that sum(x) == 1.</p>

<p>x = x/sum(x);
```</p>

<p>(3) 巧妙解法：逆迭代算法</p>

<p>巧妙利用Matlab中的精度误差导致原本是一个奇异矩阵的$I-A$变成一个非奇异矩阵，运行时只是会有些警告提示，但是运行结果和其他算法一样。</p>

<p><img src="http://hujiaweibujidao.github.io/images/algos/pk-9.png" alt="image" /></p>

<p>```
function x = pagerank2(G)
% PAGERANK1  Google’s PageRank modified version 2 - hujiawei
% using inverse iteration method</p>

<p>%if nargin &lt; 3, p = .85; end
p=0.85;</p>

<p>% Eliminate any self-referential links</p>

<p>G = G - diag(diag(G));</p>

<p>% c = out-degree, r = in-degree</p>

<p>[n,n] = size(G);
c = sum(G,1);%each row’s sum
r = sum(G,2);%each col’s sum</p>

<p>% Scale column sums to be 1 (or 0 where there are no out links).</p>

<p>k = find(c~=0);
D = sparse(k,k,1./c(k),n,n);</p>

<p>% Solve (I - p<em>G</em>D)*x = e</p>

<p>e = ones(n,1);
I = speye(n,n);
% x = (I - p<em>G</em>D)\e;
delta=(1-p)/n;
A=p<em>G</em>D+delta;
x=(I-A)\e;</p>

<p>% Normalize so that sum(x) == 1.</p>

<p>x = x/sum(x);
```</p>

<p>最后，附上参考内容4中给出的一份好代码，用于模拟随机冲浪生成矩阵$G$的代码</p>

<p>```
function [U,G] = surfer(root,n)
% SURFER  Create the adjacency graph of a portion of the Web.
%    [U,G] = surfer(root,n) starts at the URL root and follows
%    Web links until it forms an adjacency graph with n nodes.
%    U = a cell array of n strings, the URLs of the nodes.
%    G = an n-by-n sparse matrix with G(i,j)=1 if node j is linked to node i.
%
%    Example:  [U,G] = surfer(‘http://www.harvard.edu’,500);
%    See also PAGERANK.
%
%    This function currently has two defects.  (1) The algorithm for
%    finding links is naive.  We just look for the string ‘http:’.
%    (2) An attempt to read from a URL that is accessible, but very slow,
%    might take an unacceptably long time to complete.  In some cases,
%    it may be necessary to have the operating system terminate MATLAB.
%    Key words from such URLs can be added to the skip list in surfer.m.</p>

<p>% Initialize</p>

<p>clf
shg
set(gcf,’doublebuffer’,’on’)
axis([0 n 0 n])
axis square
axis ij
box on
set(gca,’position’,[.12 .20 .78 .78])
uicontrol(‘style’,’frame’,’units’,’normal’,’position’,[.01 .09 .98 .07]);
uicontrol(‘style’,’frame’,’units’,’normal’,’position’,[.01 .01 .98 .07]);
t1 = uicontrol(‘style’,’text’,’units’,’normal’,’position’,[.02 .10 .94 .04], …
   ‘horiz’,’left’);
t2 = uicontrol(‘style’,’text’,’units’,’normal’,’position’,[.02 .02 .94 .04], …
   ‘horiz’,’left’);
slow = uicontrol(‘style’,’toggle’,’units’,’normal’, …
   ‘position’,[.01 .24 .07 .05],’string’,’slow’,’value’,0);
quit = uicontrol(‘style’,’toggle’,’units’,’normal’, …
   ‘position’,[.01 .17 .07 .05],’string’,’quit’,’value’,0);</p>

<p>U = cell(n,1);
hash = zeros(n,1);
G = logical(sparse(n,n));
m = 1;
U{m} = root;
hash(m) = hashfun(root);</p>

<p>j = 1;
while j &lt; n &amp; get(quit,’value’) == 0</p>

<p>% Try to open a page.</p>

<p>try
      set(t1,’string’,sprintf(‘%5d %s’,j,U{j}))
      set(t2,’string’,’’);
      drawnow
      page = urlread(U{j});
   catch
      set(t1,’string’,sprintf(‘fail: %5d %s’,j,U{j}))
      drawnow
      continue
   end
   if get(slow,’value’)
      pause(.25)
   end</p>

<p>% Follow the links from the open page.</p>

<p>for f = findstr(‘http:’,page);</p>

<pre><code>  % A link starts with 'http:' and ends with the next quote.

  e = min([findstr('"',page(f:end)) findstr('''',page(f:end))]);
  if isempty(e), continue, end
  url = deblank(page(f:f+e-2));
  url(url&lt;' ') = '!';   % Nonprintable characters
  if url(end) == '/', url(end) = []; end

  % Look for links that should be skipped.

  skips = {'.gif','.jpg','.pdf','.css','lmscadsi','cybernet', ...
           'search.cgi','.ram','www.w3.org', ...
           'scripts','netscape','shockwave','webex','fansonly'};
  skip = any(url=='!') | any(url=='?');
  k = 0;
  while ~skip &amp; (k &lt; length(skips))
     k = k+1;
     skip = ~isempty(findstr(url,skips{k}));
  end
  if skip
     if isempty(findstr(url,'.gif')) &amp; isempty(findstr(url,'.jpg'))
        set(t2,'string',sprintf('skip: %s',url))
        drawnow
        if get(slow,'value')
           pause(.25)
        end
     end
     continue
  end

  % Check if page is already in url list.

  i = 0;
  for k = find(hash(1:m) == hashfun(url))';
     if isequal(U{k},url)
        i = k;
        break
     end
  end

  % Add a new url to the graph there if are fewer than n.

  if (i == 0) &amp; (m &lt; n)
     m = m+1;
     U{m} = url;
     hash(m) = hashfun(url);
     i = m;
  end

  % Add a new link.

  if i &gt; 0
     G(i,j) = 1;
     set(t2,'string',sprintf('%5d %s',i,url))
     line(j,i,'marker','.','markersize',6)
     drawnow
     if get(slow,'value')
        pause(.25)
     end
  end    end
</code></pre>

<p>j = j+1;
end
delete(t1)
delete(t2)
delete(slow)
set(quit,’string’,’close’,’callback’,’close(gcf)’,’value’,0)</p>

<p>%————————</p>

<p>function h = hashfun(url)
% Almost unique numeric hash code for pages already visited.
h = length(url) + 1024*sum(url);
```</p>

<h3 id="mapreducepagerank">4.MapReduce框架下PageRank算法的实现</h3>

<p>利用前面wiki上的迭代(或者幂法)的思想来实现MapReduce框架下PageRank算法很简单，可以先阅读下参考内容5。</p>

<p>这篇文章<a href="http://michaelnielsen.org/blog/using-mapreduce-to-compute-pagerank/">using-mapreduce-to-compute-pagerank</a>更加详细，可以参考</p>

<p>以下是我的大数据的一次作业，要求是参考wiki上的简便算法，实现MapReduce框架下的PageRank算法。给的数据集是Twitter的用户之间的关系，可以看做是网页之间的关系，但是助教没要求写代码以及运行这个数据集(有1G多)，所以下面只是一个Python版本的理想可行版本，并没有通过实际大数据集的验证，另外，博主暂时还不太会Python的mapreduce框架中的一些函数，所以实现的是一个简明的可以测试的PageRank算法。</p>

<h4 id="section-2">1.输入输出格式</h4>

<p><strong>map函数的输入是&lt;节点，从该节点引出的边列表&gt;，其中节点是一个类，包含了其当前的pagerank值，输出是&lt;节点，反向节点pagerank值/反向节点引出边的总数&gt;；</strong></p>

<p><strong>reduce函数的输入是&lt;节点，反向节点pagerank值/反向节点引出边的总数&gt;，输出是&lt;节点，从该节点引出的边列表&gt;，其中节点包含了其更新后的pagerank值。</strong></p>

<p>伪代码： [一时犯二写了个英文形式的 ]</p>

<p>```
process the data to the form of {node i:[its adjacent node list],…}
while the sum of difference between the last two pagerank values &lt; threshold
	map({node i:[its adjacent node list],…}):
	    map_output={}
	    for every node j in adjacent node list:
	        put or sum up {j:(i, PageRank(i)/length(adjacent node list))} into map_output
	    return map_output</p>

<pre><code>reduce(map_output):
    reduce_output={}
    for every entry {j:(i, PageRank(i)/length(adjacent node list))} in map_output:
        put or sum up all values pagerank values for node j with its adjacent node list into reduce_output
    return reduce_output
</code></pre>

<p>```</p>

<h4 id="section-3">2.示例演示</h4>

<p>假设用户1，2，3，4是如下图所示的关系：</p>

<p><img src="http://hujiaweibujidao.github.io/images/others/pagerankdemo.png" alt="image" /></p>

<p>假设有2个mapper(A和B)和1个reducer(C)，初始时4个节点的pagerank值都是0.25</p>

<p>其中，关于用户1和2的数据被mapperA读取并处理，关于用户3和4的数据被mapperB读取并处理 [经验证，即使一个用户的数据是由不同的mapper来读取的，最终收敛到的结果差不多]</p>

<p>map的输入输出结果如下：</p>

<p><img src="http://hujiaweibujidao.github.io/images/others/mapper.png" alt="image" /></p>

<p>reduce的输入输出结果如下，输入是2个mapper的输出，输出的结果中更新了节点的pagerank值</p>

<p><img src="http://hujiaweibujidao.github.io/images/others/reducer.png" alt="image" /></p>

<p>reducer处理完了之后又将它的结果输入给mapper处理，直到迭代的次数超过了设定值或者两次迭代之后得到的所有节点的pagerank值之差的总和(也可以是取二范数)小于设定的阈值。</p>

<h4 id="section-4">3.示例的实验结果</h4>

<p>(1)首先是使用Matlab采用幂法的方式计算出在p=1.0的情况下示例得到的结果 [它的主要作用是验证后面python版本的正确性]</p>

<p>matlab源码如下：</p>

<p>```
n=4;
i=[2 3 4 3 4 4 1 2];
j=[1 1 1 2 2 3 3 4];
G=sparse(i,j,1,n,n);</p>

<p>[n,n] = size(G);
for j = 1:n
   L{j} = find(G(:,j));
   c(j) = length(L{j});
end</p>

<p>% Power method
p=1.0;
delta = (1-p)/n;
x = ones(n,1)/n;
z = zeros(n,1);
cnt = 0;
while max(abs(x-z)) &gt; .0001
   z = x;
   x = zeros(n,1);
   for j = 1:n
      if c(j) == 0
         x = x + z(j)/n;
      else
         x(L{j}) = x(L{j}) + z(j)/c(j);
      end
   end
   x = p*x + delta;
   cnt = cnt+1;
end
sprintf(‘pagerank result:’)
x
```</p>

<p>结果为：</p>

<p><code>
0.1072
0.3571
0.2143
0.3214
</code></p>

<p>(2)matlab版本的page rank没有采用mapreduce的思想进行迭代，所以我另外写了一个python版本的利用mapreduce思想实现的pagerank算法(注：我并没有使用python的map和reduce函数去实现，而是使用更加容易明白的实现)，使用的阈值为0.0001，最多迭代的次数为100次。</p>

<p>```
# coding=utf-8</p>

<p><strong>author</strong> = ‘hujiawei’
<strong>doc</strong> = ‘pagerank mapreduce’</p>

<p>class Node:
    def <strong>init</strong>(self,id,pk):
        self.id=id
        self.pk=pk</p>

<p>def pk_map(map_input):
    map_output={}
    for node,outlinks in map_input.items():
        for link in outlinks:
            size=len(outlinks)
            if link in map_output:
                map_output[link]+=(float)(node.pk)/size
            else:
                map_output[link]=(float)(node.pk)/size
    return map_output</p>

<p>def pk_reduce(reduce_input):
    for result in reduce_input:
        for node,value in result.items():
            node.pk+=value</p>

<p>def pk_clear(nodes):
    for node in nodes:
        node.pk=0</p>

<p>def pk_last(nodes):
    lastnodes=[]
    for node in nodes:
        lastnodes.append(Node(node.id,node.pk))
    return lastnodes</p>

<p>def pk_diff(nodes,lastnodes):
    diff=0
    for i in range(len(nodes)):
        print(‘node pk %f, last node pk %f ‘ % (nodes[i].pk, lastnodes[i].pk))
        diff+=abs(nodes[i].pk-lastnodes[i].pk)
    return diff</p>

<p>def pk_test1():
    node1 = Node(1, 0.25)
    node2 = Node(2, 0.25)
    node3 = Node(3, 0.25)
    node4 = Node(4, 0.25)
    nodes = [node1, node2, node3, node4]
    threshold = 0.0001
    max_iters = 100</p>

<pre><code>for iter_count in range(max_iters):
    iter_count += 1
    lastnodes=pk_last(nodes)
    print('============ map count %d =================' % (iter_count))
    in1 = {node1: [node2, node3, node4], node2: [node3, node4]}
    in2 = {node3: [node1, node4], node4: [node2]}

    mapout1 = pk_map(in1)
    mapout2 = pk_map(in2)

    for node, value in mapout1.items():
        print str(node.id) + ' ' + str(value)

    for node, value in mapout2.items():
        print str(node.id) + ' ' + str(value)

    print('============ reduce count %d =================' % (iter_count))

    reducein = [mapout1, mapout2]
    pk_clear(nodes)
    pk_reduce(reducein)

    for node in nodes:
        print str(node.id) + ' ' + str(node.pk)

    diff=pk_diff(nodes,lastnodes)
    if diff &lt; threshold:
        break
</code></pre>

<p>if <strong>name</strong> == ‘<strong>main</strong>’:
    pk_test1()
```</p>

<p>得到的结果为如下，总共迭代了15次</p>

<p><code>
1 0.107138774577
2 0.35712924859
3 0.214296601128
4 0.321435375705
</code></p>

<p>上面的结果和Matlab用幂法得到的pagerank值差别很小，可以认为是正确的，所以说明了使用这种mapreduce输入输出格式的正确性。</p>

<p>OK，差不多了，希望对需要理解PageRank算法的人有帮助！ :-) </p>

]]></content>
  </entry>
  
</feed>
