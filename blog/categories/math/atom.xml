<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: math | Hujiawei Bujidao]]></title>
  <link href="http://hujiaweibujidao.github.io/blog/categories/math/atom.xml" rel="self"/>
  <link href="http://hujiaweibujidao.github.io/"/>
  <updated>2014-11-26T13:43:14+08:00</updated>
  <id>http://hujiaweibujidao.github.io/</id>
  <author>
    <name><![CDATA[hujiawei]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[SS 4-Hypothesis Testing]]></title>
    <link href="http://hujiaweibujidao.github.io/blog/2014/05/19/statistics-summary-4/"/>
    <updated>2014-05-19T23:00:00+08:00</updated>
    <id>http://hujiaweibujidao.github.io/blog/2014/05/19/statistics-summary-4</id>
    <content type="html"><![CDATA[<p><strong><center>统计学那些事 Things of Statistics</center></strong>
<strong><center>逸夫图书馆, 2014/5/19</center></strong></p>

<hr />

<h4 id="center-center"><center>第四部分 假设检验</center></h4>

<p>1.假设：一般假设就是一个“猜想”，它表述问题的一般陈述。假设检验是用于样本，然后才将结论一般化推广到总体中。</p>

<p>2.零假设(null hypothesis=$H_{0}$，或叫原假设)：它一般表示“正在研究的两个变量无关或者没有差异”这样的命题。例如，三年级学生的记忆力考试成绩与四年级学生记忆力考试成绩之间没有差异。</p>

<p><strong>(1)零假设是研究的起点，因为在没有信息的情况下，零假设就被看作是可以接受的真实状态。在这种假设下，我们认为观测到的效应是由偶然因素造成的。</strong></p>

<p><strong>(2)零假设也是研究的基准，也就是说在零假设成立的情况下，计算统计量，然后进行假设检验。这就类似反证法的思想。</strong></p>

<p>3.研究假设(research hypothesis=alternate hypothesis，或叫备择假设)：与零假设相对立的，认为变量之间有关系的假设。</p>

<p>研究假设分为有方向和无方向两种研究假设。无方向研究假设命题例子：三年级学生的记忆力考试成绩与四年级学生记忆力考试成绩之间有差异。有方向研究假设命题例子：三年级学生的记忆力考试成绩低于四年级学生记忆力考试成绩。</p>

<p>讨论有无方向的另一种形式是讨论单尾检验(one-tailed test)和双尾检验(two-tailed test)。</p>

<p><strong>零假设与研究假设的区别：</strong></p>

<p><strong>(1)零假设表示两个变量没有差异或者没有关系，研究假设表示它们有关系或者有差异；</strong></p>

<p><strong>(2)零假设对应的是总体，而研究假设对应的是样本。我们是从总体中取出一部分样本进行检验，将得到的结论推广到总体中。</strong></p>

<p><strong>(3)因为总体不能直接检验(不现实，不经济或者不可能)，所以零假设只能间接检验，研究假设则可以直接检验。</strong></p>

<p>[To be Continued…]</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SS 3-Multiple Random Variables and its Distribution]]></title>
    <link href="http://hujiaweibujidao.github.io/blog/2014/05/19/statistics-summary-2-2/"/>
    <updated>2014-05-19T20:40:00+08:00</updated>
    <id>http://hujiaweibujidao.github.io/blog/2014/05/19/statistics-summary-2-2</id>
    <content type="html"><![CDATA[<p><strong><center>统计学那些事 Things of Statistics</center></strong>
<strong><center>逸夫图书馆, 2014/5/19</center></strong></p>

<hr />

<h4 id="center-center"><center>第三部分 分布之多维随机变量及其分布</center></h4>

<p>很多情况下我们遇到的都是多维的随机变量，比如，对一个地区的儿童进行抽样统计，观察他们的身高H和体重W，样本空间S就是该地区的儿童，身高H和体重W都是定义在S上的随机变量，这里向量(H,W)就构成了二维随机向量(或者二维随机变量)，前面两节讨论的分布都是一维随机变量的分布。二维随机变量(X,Y)的性质不仅和X及Y的性质有关，还和它们的相关性有关，也就是前面提到的相关系数！</p>

<p>1.二维随机变量(X,Y)的分布函数(或者叫联合分布函数)</p>

<script type="math/tex; mode=display">F(x,y)=P(X \le x \cap Y \le y)=P(X \le x, Y \le y)</script>

<p>如果将二维随机变量看作是平面上随机点的坐标的话，那么分布函数F(x,y)在点(x,y)处的函数值就是以点(x,y)为顶点而位于该点左下方的无穷矩形域内的概率。</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/mul_1.png" alt="image" /></p>

<p>分布函数的性质：</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/mul_2.png" alt="image" /></p>

<p>如果(X,Y)只有有限对可取的值那么就是二维离散型随机变量(X,Y)，它的分布律(或者随机变量X和Y的联合分布律)：</p>

<script type="math/tex; mode=display">P(X=x_{i},Y=y_{j})=p_{ij},i,j=1,2,..., \quad (\Sigma_{i=0}^{\infty}\Sigma_{j=0}^{\infty}p_{ij}=1)</script>

<p>则有：<script type="math/tex">F(x,y)=P(X \le x, Y \le y)=\Sigma_{x \le x_{i}}\Sigma_{y \le y_{j}}p_{ij}</script></p>

<p>如果(X,Y)的值是由非负函数f(x,y)确定的，那么就是二维连续型随机变量(X,Y)，它的概率密度函数(或者随机变量X和Y的联合概率密度函数)：</p>

<script type="math/tex; mode=display">F(x,y)=\int_{-\infty}^{x_{i}}\int_{-\infty}^{y_{j}}f(u,v)dudv</script>

<p>二维连续性随机变量的性质：</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/mul_3.png" alt="image" /></p>

<p>2.边缘分布</p>

<p>边缘分布函数，边缘分布律，边缘概率密度</p>

<p><strong>边缘分布其实就是指，对于一个多维随机变量来说，它具有一个联合分布律(或者联合概率密度)，但是如果我们只考虑它其中的某一个随机变量的话，那么这个随机变量的分布律(或者概率密度)就是边缘分布律(或者边缘概率密度)了。</strong></p>

<p>二维连续型随机变量(X,Y)对X的边缘概率密度实际上就是对y进行积分<script type="math/tex">f_{X}(x)=\int_{-\infty}^{\infty}f(x,y)dy</script>，同理，对Y的边缘概率密度实际上就是对x进行积分<script type="math/tex">f_{Y}(y)=\int_{-\infty}^{\infty}f(x,y)dx</script>。</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/cond_1.png" alt="image" />
<img src="http://hujiaweibujidao.github.io/images/math/cond_2.png" alt="image" /></p>

<p>为什么叫边缘分布呢? 其实是因为一般边缘分布的数值写在联合分布律的表格边缘而已。</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/cond_3.png" alt="image" /></p>

<p>下面我们看下二维正态分布，这是一个很重要的多维随机变量分布，我们从中可以得到一些重要的结论。</p>

<p><strong>二维正态分布的两个边缘分布都是一维正态分布，给以这两个一维正态分布不同的参数会得到不同的二维正态分布，所以，已知关于X和Y的边缘分布，并不能确定X和Y的联合分布。</strong></p>

<p><img src="http://hujiaweibujidao.github.io/images/math/cond_4.png" alt="image" />
<img src="http://hujiaweibujidao.github.io/images/math/cond_5.png" alt="image" /></p>

<p>上面的参数$\rho$是随机变量X和Y的相关系数，后面还会详细介绍。</p>

<p>3.条件分布</p>

<p><strong>因为是多维随机变量，那么自然可以假定其中某一个随机变量为某个固定的值，在这种情况下我们再去看其他随机变量的分布那就是条件分布了。</strong></p>

<p>二维离散型随机变量的条件分布律</p>

<script type="math/tex; mode=display"> P(X=x_{i} \| Y=y_{j}) = \frac{P(X=x_{i},Y=y_{j})}{P(Y=y_{j})} </script>

<p><img src="http://hujiaweibujidao.github.io/images/math/cond_6.png" alt="image" /></p>

<p>二维连续型随机变量的条件概率密度</p>

<script type="math/tex; mode=display"> P(X=x_{i} \| Y=y_{j}) = \int_{-\infty}^{x}\frac{f(x,y)}{f_{Y}(y))}dx </script>

<p><img src="http://hujiaweibujidao.github.io/images/math/cond_8.png" alt="image" />
<img src="http://hujiaweibujidao.github.io/images/math/cond_9.png" alt="image" /></p>

<p>举例说明连续型条件概率密度的计算</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/cond_10.png" alt="image" /></p>

<p>4.相互独立的随机变量</p>

<p>前面我们提到随机变量之间可能存在相关性，那自然也有不存在相关性的随机变量，即相互独立的随机变量。很显然，如果随机变量(X,Y)的联合分布等于边缘分布的乘积那么就说明随机变量X和Y是相互独立的。</p>

<p>对于连续型随机变量(X,Y)来说，X和Y是相互独立的的条件是<script type="math/tex">f(x,y)=f_{X}(x)f_{Y}(y)</script>。</p>

<p>对于离散型随机变量(X,Y)来说，X和Y是相互独立的的条件是对于所有可能的<script type="math/tex">(x_{i},y_{j})</script>对，都有<script type="math/tex">P(X=x_{i},Y=y_{j})=P(X=x_{i})P(Y=y_{j})</script>。</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/ind_2.png" alt="image" /></p>

<p>如果从期望和方差的角度来看独立性的话，那么就有<script type="math/tex">E(XY)=E(X)E(Y), D(X+Y)=D(X)+D(Y)</script>，也就是随机变量XY的期望是X和Y的期望的乘积，随机变量(X+Y)的方差是X和Y的方差之和。如果X和Y是相互独立的，那么它们的协方差Cov(X,Y)=0。</p>

<script type="math/tex; mode=display">E(XY)=\int_{-\infty}^{\infty}xyf(x)f(y)dxdy=\int_{-\infty}^{\infty}xyf_{X}(x)f_{Y}(y)dxdy=\int_{-\infty}^{\infty}xf_{X}(x)dx\int_{-\infty}^{\infty}yf_{Y}(y)dy=E(X)E(Y)</script>

<p>一般情况下，$D(X+Y)=D(X)+D(Y)+2Cov(X,Y)$，如果X和Y相互独立的话，Cov(X,Y)=0，则有$D(X+Y)=D(X)+D(Y)$。</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/ind_4.png" alt="image" /></p>

<p>对于前面的二维正态随机变量，随机变量X和Y是相互独立的前提条件是它们的相关系数$\rho=0$(前面还证明过这个相关系数和X与Y的协方差相同，这就说明对于二维正态随机变量来说，它们的相关性和独立性是等价的，推广到n维正态随机变量也是如此)</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/ind_3.png" alt="image" /></p>

<p>二维随机变量中的独立性可以很容易地推广到n维随机变量上。</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/ind_1.png" alt="image" /></p>

<p>5.随机变量函数的分布</p>

<p>有些时候我们需要处理的随机变量是几个随机变量形成的函数，这个时候它的分布是怎样的呢？
这类函数比较多，比如求和，乘积，商，最大值或者最小值，这里只说明其中最重要的一个和函数的分布。</p>

<p>从中我们得到一个结论：有限个相互独立的正态随机变量的线性组合仍然是服从正态分布。</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/fun_1.png" alt="image" /></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SS 3-Continuous Distribution]]></title>
    <link href="http://hujiaweibujidao.github.io/blog/2014/05/19/statistics-summary-2-1/"/>
    <updated>2014-05-19T20:30:00+08:00</updated>
    <id>http://hujiaweibujidao.github.io/blog/2014/05/19/statistics-summary-2-1</id>
    <content type="html"><![CDATA[<p><strong><center>统计学那些事 Things of Statistics</center></strong>
<strong><center>逸夫图书馆, 2014/5/19</center></strong></p>

<hr />

<h4 id="center-center"><center>第三部分 分布之连续型随机变量分布</center></h4>

<p>连续型随机变量，概率密度及它的性质</p>

<p>函数f(x)是概率密度函数，函数F(x)是分布函数，两者都是连续函数。</p>

<script type="math/tex; mode=display">F(x)=\int_{-\infty}^{x}f(t)dt</script>

<p><a href="http://hujiaweibujidao.github.io/images/math/cont0.png">查看定义1</a>
<a href="http://hujiaweibujidao.github.io/images/math/cont1.png">查看定义2</a></p>

<p>关于连续型随机变量X对于任意一个指定实数值k的概率都是0，即$p(X=k)=0$的解释</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/cont2.png" alt="image" /></p>

<p>(1)均匀分布(uniform distribution)</p>

<p>连续型随机变量X在区间(a,b)均匀分布，它的期望是$E=\frac{a+b}{2}$，也就是期望就是区间(a,b)的中点，它的方差是$D=\frac{(b-a)^{2}}{12}$，用$D=E(X^{2})-E^{2}$去证明方便些。</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/cont_uniform.png" alt="image" /></p>

<p>(2)指数分布(exponential distribution)</p>

<p>指数分布有一个参数$\theta$，它的期望就是$\theta$，方差是$\theta^{2}$，而且它具有<strong>无记忆性</strong>。</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/cont_exp1.png" alt="image" />
<img src="http://hujiaweibujidao.github.io/images/math/cont_exp2.png" alt="image" /></p>

<p>证明它的期望是$\theta$，方差是$\theta^{2}$:</p>

<script type="math/tex; mode=display">E(X)=\int_{-\infty}^{\infty}xf(x)dx = \int_{0}^{\infty}x \frac{1}{\theta} e^{- \frac{x}{\theta}}dx = \int_{0}^{\infty}xd(-e^{- \frac{x}{\theta}}) = [-x e^{- \frac{x}{\theta}}]_{0}^{\infty} +  \int_{0}^{\infty} e^{- \frac{x}{\theta}}dx = \theta</script>

<script type="math/tex; mode=display">E(X^{2})=\int_{-\infty}^{\infty}x^{2}f(x)dx = \int_{0}^{\infty}x^{2} \frac{1}{\theta} e^{- \frac{x}{\theta}}dx = \int_{0}^{\infty}x^{2}d(-e^{- \frac{x}{\theta}}) = [-x^{2} e^{- \frac{x}{\theta}}]_{0}^{\infty} +  \int_{0}^{\infty} 2x e^{- \frac{x}{\theta}}dx = 2\theta^{2}</script>

<script type="math/tex; mode=display">D(X)=E(X^{2})-[E(X)]^{2}=\theta^{2}</script>

<p>《统计思维》对指数分布的解释：举例来说，<strong>观察一系列事件之间的间隔时间，若事件在每个时间点发生的概率相同，那么间隔时间的分布就近似指数分布</strong>(也就是前面的无记忆性)。</p>

<p>指数分布的CDF如下，此时$\lambda=\frac{1}{\theta}$</p>

<script type="math/tex; mode=display">
CDF(x)=1-e^{-\lambda x}
</script>

<p>参数$\lambda$决定了指数分布的形状，通常，指数分布的均值是$\frac{1}{\lambda}$，中位数是$\frac{log(2)}{\lambda}$。下图为$\lambda=2$的指数分布图：</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/edcdf.png" alt="image" /></p>

<p>如何判断一个分布是否是指数分布呢？一种办法是画出取对数之后的互补累积分布函数(CCDF=Complementary CDF=1-CDF(x))，CCDF是一条斜率为$-\lambda$的直线，原因如下：</p>

<script type="math/tex; mode=display">
y=CCDF(x)=1-CDF(x)=e^{-\lambda x} \quad => \quad log(y)=-\lambda x
</script>

<p>(3)正态分布(normal distribution)</p>

<p>正态分布，又叫高斯分布，是最常用的分布。其中$x=\mu$是函数f(x)的驻点，$x=\mu+\sigma,  x=\mu-\sigma$是函数f(x)的拐点，这个可能不太好计算，可用下面的Matlab代码进行验证。</p>

<p><code>matlab
clear,clc;
syms x;
syms u;
syms r;
syms p;
f=1 / (sqrt(2*p)*r) * exp(-1 * (x-u)^2 / (2*r^2) );
pretty(f)
f1=diff(f, 1) %一阶导数
pretty(f1)
f2=diff(f, 2) %二阶导数
pretty(f2)
solve(f1) %u
solve(f2) %u+r u-r
</code></p>

<p><img src="http://hujiaweibujidao.github.io/images/math/cont_normal1.png" alt="image" />
<img src="http://hujiaweibujidao.github.io/images/math/cont_normal2.png" alt="image" /></p>

<p>关于标准正态分布，即参数为$\mu=0, \sigma=1$的正态分布，它的分布函数值已经制作成表格，可以方便进行查看，其他非标准正态分布可以通过一个线性变换转换成标准正态分布。</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/cont_normal4.png" alt="image" /></p>

<p>在证明其概率密度总和为1时利用了一个重要的积分$\int_{-\infty}^{\infty} e^{\frac{t^{2}}{2}} dt = \sqrt{2 \pi}$，它的证明可以转换成二重积分然后通过极坐标计算出来。完整详细的证明可以参考下面：</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/inte1.png" alt="image" />
<img src="http://hujiaweibujidao.github.io/images/math/inte2.png" alt="image" /></p>

<p>《统计思维》对正态分布的解释：对于正态分布的CDF还没有一种准确的表达，最常用的一种形式是以误差函数(error function)来表示，它是一个特殊的函数，表示为erf(x)，在Matlab中内置了函数<code>erf</code>，对它的说明为erf函数是对参数为$\mu=0, \sigma=\frac{1}{2}$的正态分布的二重积分，有兴趣可以去计算一下，得到的结果如下：</p>

<script type="math/tex; mode=display">
CDF(x)=\frac{1}{2}[1+erf(\frac{x-\mu}{\sigma \sqrt{2}})] \quad erf(x)=\frac{2}{\sqrt{\pi}}\int_{0}^{x}e^{-t^{2}}dt
</script>

<p>其中，参数$\mu$和$\sigma$分别决定了正态分布的均值和标准差。下图为$\mu=2.0$和$\sigma=0.5$的正态分布的CDF图：[呈现明显的S型]</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/ndcdf.png" alt="image" /></p>

<p>根据大数定理，当我们处理大样本数据集(超过30个数据)，并且重复地从总体中抽取样本时，得到的数值分布就接近正态分布曲线。正态分布以均值为中心完全对称。</p>

<p>关于正态分布有一个重要的结论，对任何数值分布来说(不论它的均值和标准差)，只要数值是正态分布，那么几乎100%的数值都分布在均值的-3到3个标准差之间。下面是正态曲线下数值的分布情况：</p>

<!--
![image](http://hujiaweibujidao.github.io/images/math/nd.png)
-->

<p><img src="http://hujiaweibujidao.github.io/images/math/cont_normal3.png" alt="image" /></p>

<p>从中可以看出，<strong>在距离均值1个标准差之间大概有34%的数值分布，在1个标准差和2个标准差之间大概有13%的数值分布，在2个标准差和3个标准差之间大概有2.1%的数值分布。</strong></p>

<p>通过这个图我么可以得到一个经典的<strong>3$\sigma$法则</strong>，又叫<strong>68-95-99法则</strong></p>

<p><strong>在$(\mu - \sigma, \mu + \sigma)$之间大概有68.26%的数据分布，在$(\mu - 2\sigma, \mu + 2\sigma)$之间大概有95.44%的数据分布，在$(\mu - 3\sigma, \mu + 3\sigma)$之间大概有99.74%的数据分布。</strong></p>

<p>$\alpha$分位点的概念：对于标准正态分布X~N(0,1)，满足<script type="math/tex">P(X>Z_{\alpha})=\alpha</script>的<script type="math/tex">Z_{\alpha}</script>称为$\alpha$分位点，且有<script type="math/tex">Z_{-1\alpha}=Z_{\alpha}</script>。</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/cont_normal5.png" alt="image" /></p>

<p><a href="http://wikipedia.org/wiki/Log-normal_distribution">对数正态分布 on wiki</a>：如果一组数据取对数之后服从正态分布，那么我们就称其服从对数正态分布。对数正态分布的 CDF 跟正态分布一样, 只是用 logx 代替原来的 x:</p>

<script type="math/tex; mode=display">
CDF_{lognormal}(x) = CDF_{normal}(log x)
</script>

<p>对数正态分布的均值与标准差不再是是$\mu$和$\sigma$了。可以证明，成人体重的分布是近似对数正态的。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SS 3-Discrete Distribution]]></title>
    <link href="http://hujiaweibujidao.github.io/blog/2014/05/19/statistics-summary-2/"/>
    <updated>2014-05-19T20:00:00+08:00</updated>
    <id>http://hujiaweibujidao.github.io/blog/2014/05/19/statistics-summary-2</id>
    <content type="html"><![CDATA[<p><strong><center>统计学那些事 Things of Statistics</center></strong>
<strong><center>逸夫图书馆, 2014/5/19</center></strong></p>

<hr />

<h4 id="center-center"><center>第三部分 分布之离散型随机变量分布</center></h4>

<p>1.概率质量函数PMF(Probability Mass Function)</p>

<p>数据集的数据值到它的概率的映射函数。直方图是各个值出现的频数，如果将频数除以样本总数，得到概率，归一化之后的直方图就是PMF。</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/pmf.png" alt="image" /></p>

<p>2.累积分布函数CDF(Cumulative Distribution Function)</p>

<p>数据集的数据值到它在分布中概率的累积值的映射函数。PMF和CDF在国内的教材中并没有这样提过，但是在国外的很多统计书中都有，所以还是比较重要的，拿出来介绍下。</p>

<p>例如，CDF(0) = 0; CDF(1) = 0.2; CDF(2) = 0.6; CDF(3) = 0.8; CDF(4) = 0.8; CDF(5) = 1，它的CDF图为一个阶跃函数：</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/cdf.png" alt="image" /></p>

<p>3.离散型随机变量及其分布律</p>

<p>分布律：离散型随机变量以概率1和一定的规律分布在一些离散值上</p>

<p><a href="http://hujiaweibujidao.github.io/images/math/disc0.png">查看定义</a></p>

<p>(1)0-1分布</p>

<p>随机变量X只有两个取值0和1(样本空间只有两个取值也行)，所以叫做0-1分布，它的分布律为
$P(X=0)=p, P(X=1)=q, (q=1-p)$，它的期望是$p$，方差是$p(1-p)$。</p>

<p><a href="http://hujiaweibujidao.github.io/images/math/disc_01.png">查看定义</a></p>

<p>(2)二项分布</p>

<p>二项分布的分布律为 $P(X=k)= {n \choose k} p^{k}q^{1-k}$，因为 $P(X=k)$ 刚好是 $(p+q)^{n}$ 的二项式系数，所以这个分布就叫二项分布。二项分布是从n重伯努利试验中得到的分布，所谓的伯努利试验就是指相互独立的试验，每次试验的结果要么成功要么失败(或者说某个事件要么发生要么不发生)。</p>

<p><a href="http://hujiaweibujidao.github.io/images/math/disc_binomial.png">查看定义1</a>
<a href="http://hujiaweibujidao.github.io/images/math/disc_binomial2.png">查看定义2</a></p>

<p>(3)泊松分布</p>

<p>泊松分布是一类很常用的分布，它的分布律是$P(X=k)=\frac{\lambda^{k} e^{-\lambda}}{k!},(k=0,1,2,…)$，其中有一个参数$\lambda$，参数$\lambda$的含义既是泊松分布的期望，又是它的方差，所以，只要参数$\lambda$或者期望或者方差确定了，泊松分布就确定了。泊松分布中经常需要用到的式子[$e^{\lambda}=\Sigma_{k=0}^{\infty}\frac{x^{k}}{k!}$]</p>

<p><a href="http://hujiaweibujidao.github.io/images/math/disc_pos.png">查看定义</a></p>

<p>证明它的期望是$\lambda$: </p>

<script type="math/tex; mode=display">E=\Sigma_{k=0}^{\infty}k \frac{\lambda^{k} e^{-\lambda}}{k!}=\lambda e^{-\lambda} \Sigma_{k=1}^{\infty} \frac{\lambda^{k-1}}{(k-1)!}=\lambda e^{-\lambda} e^{\lambda}=\lambda</script>

<p>证明它的方差是$\lambda$:</p>

<script type="math/tex; mode=display">D=E(X^{2})-E^{2}=E(X(X-1)-X)-E^{2}=\Sigma_{k=0}^{\infty}k(k-1) \frac{\lambda^{k} e^{-\lambda}}{k!}+\lambda-\lambda^{2}=\lambda e^{-\lambda} \Sigma_{k=1}^{\infty} \frac{\lambda^{k-1}}{(k-1)!}=\lambda^{2} e^{-\lambda} e^{\lambda}+\lambda-\lambda^{2}=\lambda</script>

<p><strong>泊松定理说明当n很大，p很小的时候，以n，p为参数的二项分布可以用参数$\lambda = np $的泊松分布进行近似！</strong>记住这个定理其实也可以方便我们记住泊松分布的分布律。</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/disc_pos2.png" alt="image" />
<img src="http://hujiaweibujidao.github.io/images/math/disc_pos3.png" alt="image" /></p>

<p>应用举例，记住后面的结论：</p>

<p><strong>当$n \ge 20,p \le 0.05$时用泊松分布近似二项分布的概率值近似效果颇佳。</strong></p>

<p><img src="http://hujiaweibujidao.github.io/images/math/disc_pos4.png" alt="image" /></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SS 2-Covariance and Correlation coefficient]]></title>
    <link href="http://hujiaweibujidao.github.io/blog/2014/05/19/statistics-summary-3/"/>
    <updated>2014-05-19T19:30:00+08:00</updated>
    <id>http://hujiaweibujidao.github.io/blog/2014/05/19/statistics-summary-3</id>
    <content type="html"><![CDATA[<p><strong><center>统计学那些事 Things of Statistics</center></strong>
<strong><center>逸夫图书馆, 2014/5/19</center></strong></p>

<hr />

<h4 id="center-center"><center>第二部分 协方差和相关关系</center></h4>

<p>1.协方差</p>

<p>随机变量X和Y的协方差Cov(X,Y)=E[X-E(X)]E[Y-E(Y)]=E(XY)-E(X)E(Y)。</p>

<p>为什么D(X+Y)=E(XY)-E(X)E(Y)?</p>

<script type="math/tex; mode=display">
D(X+Y)=E[(X+Y)^{2}]-[E(X+Y)]^{2}=E[X^{2}+Y^{2}-2XY]-[E(X)+E(Y)]^{2} \\ =E[X^{2}]-[E(X)]^{2}+E[Y^{2}]-[E(Y)]^{2}+2E(XY)-2E(X)E(Y) \\ =D(X)+D(Y)+2E(XY)-2E(X)E(Y)
</script>

<p>因为 <strong>(X+Y)=D(X)+D(Y)+2Cov(X,Y)</strong>
所以 <strong>D(X+Y)=E(XY)-E(X)E(Y)</strong></p>

<p>协方差的性质：(基本上根据上式都可以简单证明得到)</p>

<p>Cov(X,Y)=Cov(Y,X)， Cov(X,X)=D(X)，Cov(aX,bY)=abCov(X,Y)，<script type="math/tex">Cov(X_{1}+X_{2},Y)=Cov(X_{1},Y)+Cov(X_{2},Y)</script></p>

<p>如果X和Y是相互独立的，那么Cov(X,Y)=0。</p>

<p>2.相关系数(correlation coefficient)是两个变量之间<strong>线性关系</strong>的数值型指标，取值范围是[-1,1]，大于0表示正相关，小于0表示负相关，可以用散点图来直接查看相关性。<strong>根据某些不成文的规则，一般高于0.6表示强相关，低于0.4表示弱相关，中间部分表示中度相关。</strong></p>

<p>[<strong>一般说的相关系数是Pearson相关系数，它考察的变量的属性是连续的，例如年龄，体重等，如果是离散型变量那么应该使用点二列相关系数</strong>]</p>

<p>相关系数的计算，我们经常可以看到下面两种表示方式：</p>

<p>一种表示方式是：[这是从协方差和方差的角度来看的]</p>

<script type="math/tex; mode=display">
\rho_{XY}=\frac{Cov(X,Y)}{\sqrt{D(X)}\sqrt{D(Y)}}
</script>

<p>还有一种常见的表示方式是：[这是从数据本身来看的]</p>

<script type="math/tex; mode=display">
r_{XY}=\frac{n\Sigma{XY}-\Sigma{X}\Sigma{Y}}{\sqrt{[n\Sigma{X^2}-(\Sigma{X})^2][n\Sigma{Y^2}-(\Sigma{Y})^2]}}
</script>

<p>其实，很容易看出两者是等价的，因为$E(X)=\frac{\Sigma X}{n}, D(X)=\frac{\Sigma X^{2}}{n} - \frac{\Sigma X}{n^{2}}$，然后将第二个等式右边上下除以$n^{2}$即可得到第一个等式的右边。</p>

<p>举个计算的例子：</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/cor_1.png" alt="image" /></p>

<p>得到的结果为0.692，算是一个强相关</p>

<script type="math/tex; mode=display">
r_{XY}=\frac{10 \times 247-54 \times 43}{\sqrt{[10 \times 320-(54)^2][10 \times 201-(43)^2]}} = 0.692
</script>

<p>为什么说相关系数反映的只是随机变量之间的线性关系呢？</p>

<p>我们总是用关于X的线性函数a+bX去近似Y，也就是用直线a+bX去拟合Y，如何判断拟合的好坏呢？一般都是用均方误差，也就是误差值的平方的均值，然后用均方误差对a和b分别求导即可得到使得均方误差达到最小的拟合参数a和b。很有意思的一个结论就是，<strong>均值点(E(X),E(Y))一定在拟合直线a+bX上</strong>。</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/cor_2.png" alt="image" /></p>

<p>最后得到的结果是:<script type="math/tex">min E[(Y-(a+bX))^{2}]=(1-\rho_{XY}^{2})D(Y)</script>，从中可以得到很多性质：</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/cor_3.png" alt="image" /></p>

<table>
  <tbody>
    <tr>
      <td>均方误差e和相关系数$\rho$之间的关系，e是$$</td>
      <td>\rho_{XY}</td>
      <td>$$的严格单调递减函数。</td>
    </tr>
  </tbody>
</table>

<p><img src="http://hujiaweibujidao.github.io/images/math/cor_4.png" alt="image" /></p>

<p>注意两点：</p>

<p>(1)<strong>相关系数反映的是只是线性关系！如果两个变量的相关系数为0，只能说明它们没有线性关系存在，但是可能存在其他的非线性关系！不相关和相互独立是不一样的，不相关只是就线性关系来说，相互独立是就一般关系而言。</strong></p>

<p><img src="http://hujiaweibujidao.github.io/images/math/cor_5.png" alt="image" /></p>

<p>但是，有时候相关性和独立性是等价的，比如下面的二维正态分布，这是很重要的多维随机变量分布，随机变量X和Y相互独立的条件是$\rho=0$(不明白可以看下节对多维随机变量分布的介绍)，而$\rho$正好等于<script type="math/tex">\rho_{XY}</script>，且随机变量X和Y不相关的条件就是<script type="math/tex">\rho_{XY}=0</script>，所以此时相关性和独立性是等价的。[之后我会写一篇文章并通过作图的方式介绍相关系数到底是如何影响二维正态分布的数据的分布的]</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/cor_6.png" alt="image" />
<img src="http://hujiaweibujidao.github.io/images/math/cor_7.png" alt="image" /></p>

<p>(2)<strong>相关性和因果关系无关！</strong>例如，冰淇淋的消费量和犯罪率是正相关的(可以参考《爱上统计学》)，但是两者不存在任何因果关系！</p>

<p>决定系数：相关系数的平方，它表述一个变量的方差可以被另一个变量的方差来解释的百分比。(参考《爱上统计学》)</p>

<p>3.协方差矩阵</p>

<p>协方差矩阵是非常重要的内容，经典算法PCA的基础就是协方差矩阵。引入它之前，先要看下原点矩和中心距的概念</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/corm_1.png" alt="image" /></p>

<p>协方差矩阵其实就是由n维随机变量之间两两的二阶混合中心距组成的矩阵</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/corm_2.png" alt="image" /></p>

<p>还是回到我们最重要的那个二维正态随机变量，看下如何将它的概率密度转换成协方差矩阵的表示形式，继而将其推广至n维正态随机变量。</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/corm_5.png" alt="image" />
<img src="http://hujiaweibujidao.github.io/images/math/corm_4.png" alt="image" /></p>

<p>上面最后得到的n维正态随机变量的概率密度公式是在模式识别里面非常重要的，n维正态随机变量的性质如下：</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/corm_3.png" alt="image" /></p>

]]></content>
  </entry>
  
</feed>
